{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHISHING DETECTION USING MACHINE LEARNING\n",
    "# By Ifediora Okolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-17T15:47:39.449505Z",
     "iopub.status.busy": "2022-11-17T15:47:39.448802Z",
     "iopub.status.idle": "2022-11-17T15:47:39.487257Z",
     "shell.execute_reply": "2022-11-17T15:47:39.488265Z",
     "shell.execute_reply.started": "2022-11-17T15:36:05.553827Z"
    },
    "papermill": {
     "duration": 0.088778,
     "end_time": "2022-11-17T15:47:39.488554",
     "exception": false,
     "start_time": "2022-11-17T15:47:39.399776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042946,
     "end_time": "2022-11-17T15:47:39.925915",
     "exception": false,
     "start_time": "2022-11-17T15:47:39.882969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Importing some useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2022-11-17T15:48:05.180753Z",
     "iopub.status.busy": "2022-11-17T15:48:05.179678Z",
     "iopub.status.idle": "2022-11-17T15:48:07.938960Z",
     "shell.execute_reply": "2022-11-17T15:48:07.939541Z",
     "shell.execute_reply.started": "2022-11-17T15:36:34.747472Z"
    },
    "papermill": {
     "duration": 2.831525,
     "end_time": "2022-11-17T15:48:07.939698",
     "exception": false,
     "start_time": "2022-11-17T15:48:05.108173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd # use for data manipulation and analysis\n",
    "import numpy as np # use for multi-dimensional array and matrix\n",
    "\n",
    "import seaborn as sns # use for high-level interface for drawing attractive and informative statistical graphics \n",
    "import matplotlib.pyplot as plt # It provides an object-oriented API for embedding plots into applications\n",
    "%matplotlib inline \n",
    "# It sets the backend of matplotlib to the 'inline' backend:\n",
    "import plotly.express as px\n",
    "import time # calculate time \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # algo use to predict good or bad\n",
    "from sklearn.naive_bayes import MultinomialNB # nlp algo use to predict good or bad\n",
    "\n",
    "from sklearn.model_selection import train_test_split # spliting the data between feature and target\n",
    "from sklearn.metrics import classification_report # gives whole report about metrics (e.g, recall,precision,f1_score,c_m)\n",
    "from sklearn.metrics import confusion_matrix # gives info about actual and predict\n",
    "from nltk.tokenize import RegexpTokenizer # regexp tokenizers use to split words from text  \n",
    "from nltk.stem.snowball import SnowballStemmer # stemmes words\n",
    "from sklearn.feature_extraction.text import CountVectorizer # create sparse matrix of words using regexptokenizes  \n",
    "from sklearn.pipeline import make_pipeline # use for combining all prerocessors techniuqes and algos\n",
    "\n",
    "from PIL import Image # getting images in notebook\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator# creates words colud\n",
    "\n",
    "from bs4 import BeautifulSoup # use for scraping the data from website\n",
    "from selenium import webdriver # use for automation chrome \n",
    "import networkx as nx # for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\n",
    "\n",
    "import pickle# use to dump model \n",
    "\n",
    "import warnings # ignores pink warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T15:48:08.060223Z",
     "iopub.status.busy": "2022-11-17T15:48:08.059452Z",
     "iopub.status.idle": "2022-11-17T15:48:09.285974Z",
     "shell.execute_reply": "2022-11-17T15:48:09.285444Z",
     "shell.execute_reply.started": "2022-11-17T15:36:37.556961Z"
    },
    "papermill": {
     "duration": 1.289421,
     "end_time": "2022-11-17T15:48:09.286116",
     "exception": false,
     "start_time": "2022-11-17T15:48:07.996695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "phish_data = pd.read_csv('phishing_site_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T15:48:09.671334Z",
     "iopub.status.busy": "2022-11-17T15:48:09.670676Z",
     "iopub.status.idle": "2022-11-17T15:48:09.732861Z",
     "shell.execute_reply": "2022-11-17T15:48:09.733636Z",
     "shell.execute_reply.started": "2022-11-17T15:36:38.755131Z"
    },
    "papermill": {
     "duration": 0.123237,
     "end_time": "2022-11-17T15:48:09.733871",
     "exception": false,
     "start_time": "2022-11-17T15:48:09.610634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('phishing_site_urls.csv')\n",
    "data.head()\n",
    "df_shuffled = shuffle(data, random_state=42)\n",
    "data_size = 10000\n",
    "phish_data = df_shuffled[:data_size].copy()\n",
    "phish_data.replace({'good':0, 'bad':1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T15:48:10.024831Z",
     "iopub.status.busy": "2022-11-17T15:48:10.023907Z",
     "iopub.status.idle": "2022-11-17T15:48:10.031511Z",
     "shell.execute_reply": "2022-11-17T15:48:10.030976Z",
     "shell.execute_reply.started": "2022-11-17T15:36:38.823962Z"
    },
    "papermill": {
     "duration": 0.115701,
     "end_time": "2022-11-17T15:48:10.031632",
     "exception": false,
     "start_time": "2022-11-17T15:48:09.915931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_data.isnull().sum() # there is no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T15:48:10.300018Z",
     "iopub.status.busy": "2022-11-17T15:48:10.299325Z",
     "iopub.status.idle": "2022-11-17T15:48:10.324901Z",
     "shell.execute_reply": "2022-11-17T15:48:10.325347Z",
     "shell.execute_reply.started": "2022-11-17T15:36:38.885337Z"
    },
    "papermill": {
     "duration": 0.11713,
     "end_time": "2022-11-17T15:48:10.325512",
     "exception": false,
     "start_time": "2022-11-17T15:48:10.208382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a dataframe of classes counts\n",
    "label_counts = pd.DataFrame(phish_data.Label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T15:48:10.446501Z",
     "iopub.status.busy": "2022-11-17T15:48:10.445791Z",
     "iopub.status.idle": "2022-11-17T15:48:11.241427Z",
     "shell.execute_reply": "2022-11-17T15:48:11.241944Z",
     "shell.execute_reply.started": "2022-11-17T15:36:38.944693Z"
    },
    "papermill": {
     "duration": 0.857506,
     "end_time": "2022-11-17T15:48:11.242096",
     "exception": false,
     "start_time": "2022-11-17T15:48:10.384590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#visualizing target_col\n",
    "#\n",
    "fig = px.bar(label_counts, x=label_counts.index, y=label_counts.Label, color=\"Label\", title=\"Number of Good and Bad Sites\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058965,
     "end_time": "2022-11-17T15:48:11.360820",
     "exception": false,
     "start_time": "2022-11-17T15:48:11.301855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vectorization\n",
    "#### Here the words are converted into numbers or vectors. this is  methodology in Natural Language Processing used to map\n",
    "#### words or phrases from vocabulary to a corresponding vector of real numbers which is then used to find word predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.06021,
     "end_time": "2022-11-17T15:48:11.480906",
     "exception": false,
     "start_time": "2022-11-17T15:48:11.420696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### RegexpTokenizer\n",
    "* A tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = phish_data[['URL']].copy()\n",
    "y = phish_data.Label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T15:48:11.603996Z",
     "iopub.status.busy": "2022-11-17T15:48:11.603314Z",
     "iopub.status.idle": "2022-11-17T15:48:11.606788Z",
     "shell.execute_reply": "2022-11-17T15:48:11.606142Z",
     "shell.execute_reply.started": "2022-11-17T15:36:39.749491Z"
    },
    "papermill": {
     "duration": 0.06571,
     "end_time": "2022-11-17T15:48:11.606913",
     "exception": false,
     "start_time": "2022-11-17T15:48:11.541203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T15:48:11.732238Z",
     "iopub.status.busy": "2022-11-17T15:48:11.731014Z",
     "iopub.status.idle": "2022-11-17T15:48:11.766549Z",
     "shell.execute_reply": "2022-11-17T15:48:11.765950Z",
     "shell.execute_reply.started": "2022-11-17T15:36:39.758654Z"
    },
    "papermill": {
     "duration": 0.099871,
     "end_time": "2022-11-17T15:48:11.766678",
     "exception": false,
     "start_time": "2022-11-17T15:48:11.666807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(X) :\n",
    "    X['text_tokenized'] = X.URL.map(lambda t: tokenizer.tokenize(t))\n",
    "    X['text_stemmed'] = X.text_tokenized.map(lambda t: [stemmer.stem(word) for word in t])\n",
    "    X['text_sent'] = X.text_stemmed.map(lambda t: ' '.join(t))\n",
    "    features = cv.fit_transform(X.text_sent)\n",
    "    return X, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, features = prepare_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.075947,
     "end_time": "2022-11-17T15:49:25.757017",
     "exception": false,
     "start_time": "2022-11-17T15:49:25.681070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Spliting Dataset into Train and Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import re\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import deprecation\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import models, layers, backend, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# set random seed\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# other setup\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('max_colwidth', 50)\n",
    "pio.templates.default = \"presentation\"\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('phishing_site_urls.csv')\n",
    "data.head()\n",
    "df_shuffled = shuffle(data, random_state=42)\n",
    "data_size = 500000\n",
    "data = df_shuffled[:data_size].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "train_data, val_data = train_test_split(data, test_size=val_size, stratify=data['Label'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function by Ashish Kumar Behera (2021)\n",
    "def parsed_url(url):\n",
    "    # extract subdomain, domain, and domain suffix from url\n",
    "    # if item == '', fill with '<empty>'\n",
    "    subdomain, domain, domain_suffix = ('<empty>' if extracted == '' else extracted for extracted in tldextract.extract(url))\n",
    "    \n",
    "    return [subdomain, domain, domain_suffix]\n",
    "\n",
    "def extract_url(data):\n",
    "    # parsed url\n",
    "    extract_url_data = [parsed_url(url) for url in data['URL']]\n",
    "    extract_url_data = pd.DataFrame(extract_url_data, columns=['subdomain', 'domain', 'domain_suffix'])\n",
    "    \n",
    "    # concat extracted feature with main data\n",
    "    data = data.reset_index(drop=True)\n",
    "    data = pd.concat([data, extract_url_data], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_frequent_group(data, n_group):\n",
    "    # get the most frequent\n",
    "    data = data.value_counts().reset_index(name='values')\n",
    "    # scale log base 10\n",
    "    data['values'] = np.log10(data['values'])\n",
    "    \n",
    "    # calculate total values\n",
    "    # x_column (subdomain / domain / domain_suffix)\n",
    "    x_column = data.columns[1]\n",
    "    data['total_values'] = data[x_column].map(data.groupby(x_column)['values'].sum().to_dict())\n",
    "    \n",
    "    # get n_group data order by highest values\n",
    "    data_group = data.sort_values('total_values', ascending=False).iloc[:, 1].unique()[:n_group]\n",
    "    data = data[data.iloc[:, 1].isin(data_group)]\n",
    "    data = data.sort_values('total_values', ascending=False)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def plot(data, n_group, title):\n",
    "    data = get_frequent_group(data, n_group)\n",
    "    fig = px.bar(data, x=data.columns[1], y='values', color='label')\n",
    "    fig.update_layout(title=title)\n",
    "    fig.show()\n",
    "\n",
    "# extract url\n",
    "data = extract_url(data)\n",
    "train_data = extract_url(train_data)\n",
    "val_data = extract_url(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='', char_level=True, lower=False, oov_token=1)\n",
    "\n",
    "# fit only on training data\n",
    "tokenizer.fit_on_texts(train_data['URL'])\n",
    "n_char = len(tokenizer.word_index.keys())\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_data['URL'])\n",
    "val_seq = tokenizer.texts_to_sequences(val_data['URL'])\n",
    "\n",
    "print('Before tokenization: ')\n",
    "print(train_data.iloc[0]['URL'])\n",
    "print('\\nAfter tokenization: ')\n",
    "print(train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = np.array([len(i) for i in train_seq])\n",
    "sequence_length = np.percentile(sequence_length, 99).astype(int)\n",
    "print(f'Before padding: \\n {train_seq[0]}')\n",
    "train_seq = pad_sequences(train_seq, padding='post', maxlen=sequence_length)\n",
    "val_seq = pad_sequences(val_seq, padding='post', maxlen=sequence_length)\n",
    "print(f'After padding: \\n {train_seq[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value = {}\n",
    "for feature in ['subdomain', 'domain', 'domain_suffix']:\n",
    "    # get unique value\n",
    "    label_index = {label: index for index, label in enumerate(train_data[feature].unique())}\n",
    "    \n",
    "    # add unknown label in last index\n",
    "    label_index['<unknown>'] = list(label_index.values())[-1] + 1\n",
    "    \n",
    "    # count unique value\n",
    "    unique_value[feature] = label_index['<unknown>']\n",
    "    \n",
    "    # encode\n",
    "    train_data.loc[:, feature] = [label_index[val] if val in label_index else label_index['<unknown>'] for val in train_data.loc[:, feature]]\n",
    "    val_data.loc[:, feature] = [label_index[val] if val in label_index else label_index['<unknown>'] for val in val_data.loc[:, feature]]\n",
    "    \n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train_data, val_data]:\n",
    "    data.loc[:, 'Label'] = [0 if i == 'good' else 1 for i in data.loc[:, 'Label']]\n",
    "    \n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function by RAKHA KAWISWARA (2021) \n",
    "def convolution_block(x):\n",
    "    conv_3_layer = layers.Conv1D(64, 3, padding='same', activation='elu')(x)\n",
    "    conv_5_layer = layers.Conv1D(64, 5, padding='same', activation='elu')(x)\n",
    "    conv_layer = layers.concatenate([x, conv_3_layer, conv_5_layer])\n",
    "    conv_layer = layers.Flatten()(conv_layer)\n",
    "    return conv_layer\n",
    "\n",
    "def embedding_block(unique_value, size, name):\n",
    "    input_layer = layers.Input(shape=(1,), name=name + '_input')\n",
    "    embedding_layer = layers.Embedding(unique_value, size, input_length=1)(input_layer)\n",
    "    return input_layer, embedding_layer\n",
    "\n",
    "def create_model(sequence_length, n_char, unique_value):\n",
    "    input_layer = []\n",
    "    \n",
    "    # sequence input layer\n",
    "    sequence_input_layer = layers.Input(shape=(sequence_length,), name='url_input')\n",
    "    input_layer.append(sequence_input_layer)\n",
    "    \n",
    "    # convolution block\n",
    "    char_embedding = layers.Embedding(n_char + 1, 32, input_length=sequence_length)(sequence_input_layer)\n",
    "    conv_layer = convolution_block(char_embedding)\n",
    "    # entity embedding\n",
    "    entity_embedding = []\n",
    "    for key, n in unique_value.items():\n",
    "        size = 4\n",
    "        input_l, embedding_l = embedding_block(n + 1, size, key)\n",
    "        embedding_l = layers.Reshape(target_shape=(size,))(embedding_l)\n",
    "        input_layer.append(input_l)\n",
    "        entity_embedding.append(embedding_l)\n",
    "        \n",
    "    # concat all layer\n",
    "    fc_layer = layers.concatenate([conv_layer, *entity_embedding])\n",
    "    fc_layer = layers.Dropout(rate=0.5)(fc_layer)\n",
    "    \n",
    "    # dense layer\n",
    "    fc_layer = layers.Dense(128, activation='elu')(fc_layer)\n",
    "    fc_layer = layers.Dropout(rate=0.2)(fc_layer)\n",
    "    \n",
    "    # output layer\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(fc_layer)\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(), metrics.Recall()])\n",
    "    return model\n",
    "# reset session\n",
    "backend.clear_session()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# create model\n",
    "model = create_model(sequence_length, n_char, unique_value)\n",
    "\n",
    "# show model architecture\n",
    "plot_model(model, to_file='model.png')\n",
    "model_image = mpimg.imread('model.png')\n",
    "plt.figure(figsize=(75, 75))\n",
    "plt.imshow(model_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train data\n",
    "train_x = [train_seq, train_data['subdomain'], train_data['domain'], train_data['domain_suffix']]\n",
    "train_y = train_data['Label'].values\n",
    "\n",
    "# model training\n",
    "early_stopping = [EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True, mode='max')]\n",
    "history = model.fit(train_x, train_y, batch_size=64, epochs=25, verbose=1, validation_split=0.2, shuffle=True, callbacks=early_stopping)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(3, 1, subplot_titles=('loss', 'precision', 'recall'))\n",
    "\n",
    "for index, key in enumerate(['loss', 'precision', 'recall']):\n",
    "    # train score\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history.history[key]))),\n",
    "        y=history.history[key],\n",
    "        mode='lines+markers',\n",
    "        name=key\n",
    "    ), index + 1, 1)\n",
    "    \n",
    "    # val score\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history.history[f'val_{key}']))),\n",
    "        y=history.history[f'val_{key}'],\n",
    "        mode='lines+markers',\n",
    "        name=f'val {key}'\n",
    "    ), index + 1, 1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = [val_seq, val_data['subdomain'], val_data['domain'], val_data['domain_suffix']]\n",
    "val_y = val_data['Label'].values\n",
    "\n",
    "val_pred = model.predict(val_x)\n",
    "val_pred = np.where(val_pred[:, 0] >= 0.5, 1, 0)\n",
    "print(f'Validation Data:\\n{val_data.Label.value_counts()}')\n",
    "print(f'\\n\\nConfusion Matrix:\\n{confusion_matrix(val_y, val_pred)}')\n",
    "print(f'\\n\\nClassification Report:\\n{classification_report(val_y, val_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the file path with the path to your file on your Google Drive\n",
    "file_path = 'phishing_site_urls.csv'\n",
    "from sklearn.utils import shuffle\n",
    "# Loading the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()\n",
    "df_shuffled = shuffle(data, random_state=42)\n",
    "data_size = 10000\n",
    "phish_data = df_shuffled[:data_size].copy()\n",
    "phish_data.replace({'good':0, 'bad':1}, inplace=True)\n",
    "X = phish_data[['URL']].copy()\n",
    "y = phish_data.Label.copy()\n",
    "training_sizes = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X) :\n",
    "    X['text_tokenized'] = X.URL.map(lambda t: tokenizer.tokenize(t))\n",
    "    X['text_stemmed'] = X.text_tokenized.map(lambda t: [stemmer.stem(word) for word in t])\n",
    "    X['text_sent'] = X.text_stemmed.map(lambda t: ' '.join(t))\n",
    "    features = cv.fit_transform(X.text_sent)\n",
    "    return X, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, features = prepare_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [2, 4, 6, 8, 10, 12, 14, 16, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function by Vitor Gama Lemos. (2022b). \n",
    "def train_test_nn(X, y, training_percentage, hidden_units) :\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-training_percentage, stratify=y, random_state=42)\n",
    "    X_train = X_train.toarray()\n",
    "    X_test = X_test.toarray()\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1], ), name='Input-Layer'))\n",
    "    model.add(Dense(hidden_units, activation='relu', name='Hidden-Layer'))\n",
    "    model.add(Dense(1, activation='sigmoid', name='Output-Layer'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Accuracy', 'Precision', 'Recall'])\n",
    "    model.fit(X_train, y_train, batch_size = 10, epochs = 10, verbose=0)\n",
    "    score = model.evaluate(X_test, y_test, batch_size = 1, verbose=2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "accuracies_nn = []\n",
    "precisions_nn = []\n",
    "recalls_nn = []\n",
    "for ts in training_sizes :\n",
    "    a = []\n",
    "    p = []\n",
    "    r = []\n",
    "    for hn in hidden_units :\n",
    "        s = train_test_nn(features, y, ts, hn)\n",
    "        a.append(s[1])\n",
    "        p.append(s[2])\n",
    "        r.append(s[3])\n",
    "    accuracies_nn.append(a)\n",
    "    precisions_nn.append(p)\n",
    "    recalls_nn.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Evaluation parameters (ACCURACY, PRECISION and RECALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_nn_df = pd.DataFrame(accuracies_nn, columns=hidden_units, index=training_sizes*100)\n",
    "precisions_nn_df = pd.DataFrame(precisions_nn, columns=hidden_units, index=training_sizes*100)\n",
    "recalls_nn_df = pd.DataFrame(recalls_nn, columns=hidden_units, index=training_sizes*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_nn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_nn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls_nn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#References\n",
    "#(1) Malicious Web Detection with 1D CNN. (n.d.). Kaggle.com. Retrieved May 10, 2023, \n",
    "#   from https://www.kaggle.com/code/kawiswara/malicious-web-detection-with-1d-cnn\n",
    "\n",
    "#(2)Vitor Gama Lemos. (2022b). Phishing detect + NLP + Ngram + XGBoost + EDA. Kaggle. \n",
    "#   https://www.kaggle.com/code/vitorgamalemos/phishing-detect-nlp-ngram-xgboost-eda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "duration": 188.795112,
   "end_time": "2022-11-17T15:50:43.962652",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-17T15:47:35.167540",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
